# Regression Shrinkage and Selection via the Lasso (Tibshirani, 1996)

## Dissertation Alignment Context
The dissertation proposal uses a three-stage longitudinal pipeline: critical-range rectification of lagged signals into {-1,+1} indicators, L1-regularized logistic feature-and-lag selection, and anytime rule compression that converts sparse coefficients into compact m-of-K rules.

## Outline Summary of the Paper
1. **Problem setup and motivation:** The paper targets linear regression settings where subset selection is unstable and seeks a method that combines coefficient shrinkage with automatic variable selection.
2. **Lasso formulation:** It introduces the lasso as least squares with an L1 constraint (or equivalent L1-penalized objective), producing sparse coefficient vectors.
3. **Geometric intuition:** The paper explains sparsity through the geometry of the L1 constraint region, where optimization frequently lands on corners corresponding to exact zeros.
4. **Links to prior methods:** It contrasts lasso with ridge regression and subset selection, positioning lasso as an intermediate that keeps shrinkage while performing selection.
5. **Optimization perspective:** It develops practical computation through constrained optimization/Lagrange multiplier viewpoints and path-style reasoning over tuning levels.
6. **Statistical behavior discussion:** The method is analyzed in terms of bias-variance tradeoff, showing how shrinkage can reduce prediction error relative to unpenalized fits.
7. **Model selection and tuning:** It discusses choosing the regularization level (for example with prediction-error criteria), emphasizing that performance depends on the penalty level.
8. **Empirical examples:** Simulations and data examples illustrate sparse recovery behavior and predictive competitiveness versus contemporaneous alternatives.
9. **Extensions beyond Gaussian regression:** The article outlines how the L1 idea can be adapted to broader likelihood settings, motivating later generalized-model lasso work.
10. **Main contribution and impact:** The paper establishes L1 regularization as a practical sparse modeling framework that became foundational for high-dimensional statistics and machine learning.

## Relevance to the Dissertation
Regression Shrinkage and Selection via the Lasso (Tibshirani, 1996) is directly relevant because it offers a sparse-selection strategy that competes with the proposal's rectification-first approach under multicollinearity.

## Elements from This Paper to Use in the Dissertation
1. Include this method as an explicit baseline on both raw lag-expanded and rectified features.
2. Track support stability, false positives, and lag attribution quality, not only AUC/F1.
3. Reuse discussion around its core concepts to motivate when penalty-only methods are sufficient.
4. Compare runtime and memory against the rectification + L1 + anytime-rule pipeline.

## Competitive Method Assessment
This paper describes a genuine competing method family. Relative to the dissertation pipeline, it can fall short when raw lag-expanded correlation is so high that support selection remains unstable or when explicit threshold/rule semantics are required. It can excel when linear signal is strong, penalty tuning is mature, and compact logical rules are not the primary requirement.


## Dissertation Citation Traceability

- Chapter: `Introduction`; Section: `Chapter context (no explicit section)`; Line: `Chapters/01_introduction.tex:13`; Relevance: Cited to support the statement that In high-dimensional longitudinal modeling, a standard strategy is to construct a lag-expanded design matrix and apply a sparse estimator. This strategy is attractive because sparse models can retain predictive performance while reducing dimensionality . However, lag expansion also introduces strong dependence among adjacent lags and among correlated channels, creating exactly the regimes where support recovery becomes unstable and model-selection assumptions can fail . Practical outcomes include arbitrary swapping among correlated predictors, sensitivity to tuning choices, and weak reproducibility of selected supports across resamples.
- Chapter: `Background`; Section: `Chapter context (no explicit section)`; Line: `Chapters/02_background.tex:5`; Relevance: Cited to support the statement that Sparse modeling with L1 penalties is a standard approach for feature selection in high-dimensional data because it often yields compact models with practical predictive performance . In longitudinal settings, sparse selection is also used for lag attribution: selected coefficients identify not only which variables matter, but when they matter. This chapter reviews the background needed for the dissertation, including sparse regularization, support-recovery theory under dependence, interpretability requirements, and competing method families.
- Chapter: `Background`; Section: `L1-regularized models and feature selection`; Line: `Chapters/02_background.tex:10`; Relevance: Cited to support the statement that The LASSO and L1-regularized logistic regression induce sparsity by shrinking many coefficients to exactly zero, creating an embedded feature-selection mechanism inside model fitting . This property is one reason L1 methods are widely used in longitudinal pipelines: selected nonzero lag terms can provide concise, testable hypotheses about delayed effects.
- Chapter: `Related Work`; Section: `Sparse Regularization Foundations`; Line: `Chapters/03_relatedwork.tex:9`; Relevance: Cited to support the statement that L1-regularized modeling is the canonical starting point for sparse feature selection. The lasso formulation introduced by Tibshirani established a convex mechanism that performs shrinkage and variable selection simultaneously, making it practical for high-dimensional settings . In classification settings, L1-regularized logistic regression inherits the same sparsity behavior and is widely used when model compactness and feature attribution are important.
- Chapter: `Research Questions`; Section: `Chapter context (no explicit section)`; Line: `Chapters/04_resquestions.tex:5`; Relevance: Cited to support the statement that This chapter defines the core research questions that guide this work and explains why they are important. Longitudinal feature learning is targeted under three simultaneous constraints: strong dependence created by lag expansion, the need for reliable feature-lag attribution, and the need for compact human-auditable model outputs. Prior chapters established that sparse methods are attractive but not always stable under dependence, and that interpretability requirements in high-stakes settings favor inherently transparent model forms over post hoc explanation layers.
- Chapter: `Theoretical Analysis (RQ2)`; Section: `Why RQ2 matters after RQ1 evidence`; Line: `Chapters/06_rq2.tex:13`; Relevance: Cited to support the statement that For sparse selection methods, this concern is especially important. Lasso is a strong predictive tool , but model-selection consistency depends on structural conditions that may fail under multicollinearity . In other words, high AUC does not imply reliable feature recovery. RQ2 therefore asks whether rectification improves a known structural bottleneck rather than merely shifting surface metrics.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `From Rectified Sparse Models to Rule Space`; Line: `Chapters/07_rq3.tex:19`; Relevance: Cited to support the statement that where is the binarized/rectified feature and is sparse due to the L1 penalty . Let the active set be . RQ3 starts from this fitted model and asks whether we can replace the full weighted sum with a smaller rule that preserves operating performance.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `Algorithmic Pipeline > 1. Train Baseline Sparse Rectified Model`; Line: `Chapters/07_rq3.tex:39`; Relevance: Cited to support the statement that Fit L1-regularized logistic regression on the rectified matrix and compute baseline operating metrics (AUC, , sensitivity, specificity) at the chosen operating threshold. This baseline defines the reference quality floor for compression.
- Chapter: `Future Work`; Section: `Workstream A: RQ1 Empirical Strengthening > A2. Baseline expansion under multicollinearity`; Line: `Chapters/88_futurework.tex:35`; Relevance: Cited to support the statement that item L1 and elastic-net path baselines.
- Chapter: `Conclusion`; Section: `Positioning Relative to Prior Work`; Line: `Chapters/89_conclusion.tex:51`; Relevance: Cited to support the statement that This work extends, rather than replaces, prior sparse and interpretable modeling literature. Relative to classical L1 and elastic-net families , the main difference is intervention point: representation is modified before sparse optimization. Relative to grouped and ordered penalties , this work emphasizes threshold logic and lag-attribution clarity in a longitudinal context. Relative to direct rule-learning approaches , it uses sparse convex fitting as a scalable front end and performs structured rule simplification afterward.
