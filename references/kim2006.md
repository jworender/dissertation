# Blockwise Sparse Regression (Kim et al., 2006)

## Dissertation Alignment Context
The dissertation proposal uses a three-stage longitudinal pipeline: critical-range rectification of lagged signals into {-1,+1} indicators, L1-regularized logistic feature-and-lag selection, and anytime rule compression that converts sparse coefficients into compact m-of-K rules.

## Outline Summary of the Paper
1. **Motivation and scope (Section 1):** The paper extends grouped sparsity ideas to settings beyond least-squares regression, targeting block-level variable selection under general loss functions (including logistic models).
2. **Core formulation (Section 2.1):** Blockwise Sparse Regression (BSR) partitions coefficients into predefined blocks and minimizes empirical loss subject to an \(\ell_1\)-type constraint on block norms \(\sum_j \|\beta_{(j)}\| \le M\), inducing whole-block sparsity.
3. **Positioning relative to ridge and LASSO (Section 2.1):** BSR is presented as a compromise between ridge and LASSO: single-block designs recover ridge-like behavior, one-feature-per-block designs recover ordinary LASSO, and squared-loss settings align with grouped LASSO.
4. **Optimization algorithm (Section 2.2):** The paper develops a gradient-projection solver tailored to the block-norm constraint, iteratively updating gradients and projecting onto the feasible set with active-set refinement.
5. **Tuning strategy for \(M\) (Section 3):** In addition to K-fold CV, it proposes a GCV-type approximation for faster restriction-parameter selection, derived via an iteratively reweighted quadratic surrogate.
6. **Blockwise standardization (Section 4):** A block-specific orthogonalization/normalization procedure is introduced so block norms become comparable across alternative codings (e.g., different dummy-variable parameterizations).
7. **Simulation design (Section 5.1):** Three logistic-regression simulations are used to compare BSR, ridge, and LASSO under different ground-truth structures favoring block-level, individual-feature, or dense-signal patterns.
8. **Simulation findings (Sections 5.2-5.4):** BSR performs best when true signal is naturally block-structured, LASSO excels when only individual terms within blocks matter, and ridge leads when signal is broadly dense; BSR is often intermediate and more block-selective.
9. **Real-data applications (Section 6):** On German credit and breast-cancer datasets, BSR selects interpretable subsets of grouped transformed features and achieves competitive or best predictive metrics versus ridge/LASSO.
10. **Conclusions and extensions (Section 7):** The authors argue BSR is broadly applicable across generalized linear-type models, with future directions including alternative penalties, theoretical asymptotics, and further algorithmic extensions.

## Relevance to the Dissertation
Blockwise Sparse Regression (Kim et al., 2006) is directly relevant because it offers a sparse-selection strategy that competes with the proposal's rectification-first approach under multicollinearity.

## Elements from This Paper to Use in the Dissertation
1. Include this method as an explicit baseline on both raw lag-expanded and rectified features.
2. Track support stability, false positives, and lag attribution quality, not only AUC/F1.
3. Reuse discussion around statistica, sinica, blockwise to motivate when penalty-only methods are sufficient.
4. Compare runtime and memory against the rectification + L1 + anytime-rule pipeline.

## Competitive Method Assessment
This paper describes a genuine competing method family. Relative to the dissertation pipeline, it can fall short when raw lag-expanded correlation is so high that support selection remains unstable or when explicit threshold/rule semantics are required. It can excel when linear signal is strong, penalty tuning is mature, and compact logical rules are not the primary requirement.


## Dissertation Citation Traceability

- Chapter: `Background`; Section: `L1-regularized models and feature selection`; Line: `Chapters/02_background.tex:12`; Relevance: Cited to support the statement that Modern implementations make these methods computationally practical at scale: coordinate-descent path algorithms with warm starts support efficient generalized linear modeling across many tuning values . Beyond plain L1 penalties, elastic net, adaptive lasso, and group-aware variants address correlated and block-structured predictors, which is directly relevant for lagged longitudinal features.
- Chapter: `Related Work`; Section: `Group, Block, and Ordered Sparsity`; Line: `Chapters/03_relatedwork.tex:31`; Relevance: Cited to support the statement that Subsequent work extended grouped sparsity to logistic settings and improved practical optimization. Group lasso for logistic regression, blockwise sparse regression, and unified majorization-descent solvers significantly improved feasibility for high-dimensional non-orthonormal designs . These methods remain strong comparators because they encode structural assumptions directly in the objective while retaining convexity in many cases.
- Chapter: `Preliminary Studies and Evidence to Date for RQ1`; Section: `Comparison to competing feature-selection baselines`; Line: `Chapters/05_rq1.tex:53`; Relevance: Cited to support the statement that Preliminary RQ1 evidence also includes comparisons with methods intended to address redundancy or grouped dependence through different mechanisms. Group and block-structured sparse families remain important comparators because they are explicitly designed for correlated predictor settings . In these early studies, however, representation-level rectification combined with standard lasso often provides clearer lag attribution in threshold-triggered synthetic settings.
