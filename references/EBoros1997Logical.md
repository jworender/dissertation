# Logical Analysis of Numerical Data (Boros et al., 1997)

## Dissertation Alignment Context
The dissertation proposal uses a three-stage longitudinal pipeline: critical-range rectification of lagged signals into {-1,+1} indicators, L1-regularized logistic feature-and-lag selection, and anytime rule compression that converts sparse coefficients into compact m-of-K rules.

## Outline Summary of the Paper
1. **Motivation and scope (Introduction):** The paper extends Logical Analysis of Data (LAD) from binary datasets to numerical datasets by formalizing binarization via cut points and indicator variables.
2. **LAD framing and examples:** It introduces the idea of deriving logical explanations (Boolean extensions) that separate positive and negative observations, and uses concrete examples to show how different cut-point choices change model simplicity and interpretability.
3. **Core definitions and preliminaries:** The paper defines partially defined Boolean functions and extension classes (all, monotone/positive, Horn, threshold, linear, quadratic), then recalls feasibility conditions for extensions in these classes.
4. **Master binarization construction:** It develops the concept of a master partially defined Boolean function induced by all potentially useful cut points, establishing a common representation for optimization over binarizations.
5. **Existence and complexity results:** It studies when a numerical dataset admits a binarization compatible with a target logical class, proving polynomial solvability in some settings and NP-hardness in others (notably constrained one-cut-per-attribute variants).
6. **Minimum-cut-point optimization problem:** The main combinatorial problem is to minimize the number of cut points while preserving existence of an extension in a chosen function class.
7. **Integer programming formulations:** It formulates the minimization problem as compact integer programs (including set-cover style formulations for certain classes), enabling practical solution via optimization methods.
8. **Class-specific analysis:** The paper provides separate treatment and formulations for monotone, Horn, threshold, linear, and quadratic function classes, highlighting where structure can be exploited computationally.
9. **Polynomially solvable special cases:** It gives polynomial-time algorithms for selected restricted settings (including bounded dimension and certain low-dimensional monotone cases), showing that tractability depends strongly on structural assumptions.
10. **Overall conclusion:** The work establishes binarization as a rigorous optimization problem within LAD, with a clear theory-computation bridge: some cases are tractable, others are NP-hard, and integer programming offers a practical framework for real data analysis.

## Relevance to the Dissertation
Logical Analysis of Numerical Data (Boros et al., 1997) is directly relevant to the proposal's interpretable-model objective and the final anytime rule-compression stage.

## Elements from This Paper to Use in the Dissertation
1. Use this paper's interpretability framing to justify rule-first design decisions.
2. Borrow complexity metrics (rule count, rule length, transparency) for evaluation.
3. Reuse details around mathematical, programming, logical to improve model reporting and human-auditable outputs.
4. Benchmark rule quality against the proposal's anytime compression stage.

## Competitive Method Assessment
This paper is a direct competitor for interpretable-model construction. Competing rule methods may outperform the dissertation approach when their search objective matches the data regime (e.g., small categorical spaces), but can fall short under large lag-expanded continuous settings where anytime compression is computationally safer.


## Dissertation Citation Traceability

- Chapter: `Introduction`; Section: `Chapter context (no explicit section)`; Line: `Chapters/01_introduction.tex:17`; Relevance: Cited to support the statement that A second challenge is interpretability under high-stakes use. Post hoc explanations of complex models can be useful, but interpretable-model-first approaches are often preferable when transparency, auditability, and operational trust are mandatory . Rule-structured models are especially relevant because they align with human review workflows and can be validated directly by domain experts . This dissertation therefore focuses on an inherently interpretable sparse pipeline rather than black-box prediction with after-the-fact explanation.
- Chapter: `Background`; Section: `Interpretability as a design requirement`; Line: `Chapters/02_background.tex:29`; Relevance: Cited to support the statement that Rule-based models provide a natural interface for such settings because they are directly inspectable and can be validated by experts testing each feature and lag individually against specific rules. Classical logical-analysis frameworks and modern optimal-rule-list methods show that transparent rule structures can be competitive while preserving auditability . This work adopts this perspective by treating rule compression as part of model construction, not merely post-hoc explanation.
- Chapter: `Background`; Section: `Competing approaches`; Line: `Chapters/02_background.tex:40`; Relevance: Cited to support the statement that item Logic-rule and LAD-style methods explicitly target threshold semantics and human-readable decisions, yet can face combinatorial scaling pressures as feature spaces grow.
- Chapter: `Related Work`; Section: `Interpretable Modeling and Rule-Learning Literature`; Line: `Chapters/03_relatedwork.tex:51`; Relevance: Cited to support the statement that Rule-based models are central to this viewpoint. Classical Logical Analysis of Data (LAD) frameworks formalize binarization and logic-pattern extraction, including optimization-based treatment of cut-point selection and pattern construction . These methods establish the value of explicit threshold logic for decision support, but can face combinatorial scaling limits in large, highly correlated feature spaces.
- Chapter: `Related Work`; Section: `Summary of Gaps in Prior Work`; Line: `Chapters/03_relatedwork.tex:82`; Relevance: Cited to support the statement that item Rule-learning frameworks deliver transparency, but direct combinatorial optimization can be difficult to scale in high-dimensional longitudinal spaces.
- Chapter: `Research Questions`; Section: `Why RQ3 matters`; Line: `Chapters/04_resquestions.tex:36`; Relevance: Cited to support the statement that Rule-oriented model families and LAD-style traditions demonstrate the value of explicit logic structures for decision support . However, direct combinatorial search can be expensive in high-dimensional longitudinal spaces. The dissertation's question is whether an anytime compression stage can deliver much of the same interpretability benefit while preserving most of the discriminative value of the sparse baseline.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `Relationship to Adjacent Interpretable Methods`; Line: `Chapters/07_rq3.tex:82`; Relevance: Cited to support the statement that The approach is also related to Logical Analysis of Data (LAD), particularly in its use of thresholded indicators and logical structure, but it targets a different optimization path and deployment interface . In the dissertation context, this can be viewed as a bridge between sparse statistical learning and logic-level decision rules tailored to threshold-driven events.
- Chapter: `Future Work`; Section: `Workstream C: RQ3 Anytime Compression Validation > C4. Comparator rule learners`; Line: `Chapters/88_futurework.tex:91`; Relevance: Cited to support the statement that item LAD-style logical methods and implementation variants.
- Chapter: `Conclusion`; Section: `Positioning Relative to Prior Work`; Line: `Chapters/89_conclusion.tex:51`; Relevance: Cited to support the statement that This work extends, rather than replaces, prior sparse and interpretable modeling literature. Relative to classical L1 and elastic-net families , the main difference is intervention point: representation is modified before sparse optimization. Relative to grouped and ordered penalties , this work emphasizes threshold logic and lag-attribution clarity in a longitudinal context. Relative to direct rule-learning approaches , it uses sparse convex fitting as a scalable front end and performs structured rule simplification afterward.
