# Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead (Rudin, 2019)

## Dissertation Alignment Context
The dissertation proposal uses a three-stage longitudinal pipeline: critical-range rectification of lagged signals into {-1,+1} indicators, L1-regularized logistic feature-and-lag selection, and anytime rule compression that converts sparse coefficients into compact m-of-K rules.

## Outline Summary of the Paper
1. **Core thesis and scope (Abstract + Introduction):** The paper argues that for high-stakes decisions, we should prefer inherently interpretable models over post hoc explanations of black boxes because explanation layers can be misleading and unsafe.
2. **What counts as interpretability (Section 1):** It frames interpretability as domain-specific and emphasizes model-form constraints (for example sparsity, monotonicity, and structure) rather than one universal definition.
3. **Problems with explainable black boxes (Section 2):** The paper details major risks of post hoc explanation, including lack of faithfulness, potential for misleading users, and examples where explanation artifacts can misrepresent what the original model actually computes.
4. **Accuracy-versus-interpretability myth (Section 2/3 discussion):** It challenges the common claim that black boxes are always more accurate, arguing that in many structured-data tasks interpretable models can match predictive performance.
5. **Domain examples and evidence (Section 3):** It uses high-stakes applications (notably criminal justice and healthcare) and the COMPAS-vs-CORELS discussion to show that transparent models can be competitive while being auditable.
6. **Governance and policy recommendations (Section 4):** It proposes stronger accountability mechanisms, including requirements to report interpretable-model performance when deploying black-box systems in sensitive settings.
7. **Algorithmic challenges in interpretable ML (Section 5):** It identifies technical hurdles and organizes them into representative challenge families, including constructing optimal logical models, interpretable linear/additive modeling under constraints, and case-based reasoning approaches.
8. **Conclusion and supporting appendices (Section 6 + Appendices):** It closes by reinforcing the interpretability-first stance for high-stakes use and supplements the main argument with additional details on proprietary models and the accuracy-interpretability narrative.

## Relevance to the Dissertation
Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead (Rudin, 2019) is directly relevant to the proposal's interpretable-model objective and the final anytime rule-compression stage.

## Elements from This Paper to Use in the Dissertation
1. Use this paper's interpretability framing to justify rule-first design decisions.
2. Borrow complexity metrics (rule count, rule length, transparency) for evaluation.
3. Reuse details around stop, explaining, black to improve model reporting and human-auditable outputs.
4. Benchmark rule quality against the proposal's anytime compression stage.

## Competitive Method Assessment
This paper is a direct competitor for interpretable-model construction. Competing rule methods may outperform the dissertation approach when their search objective matches the data regime (e.g., small categorical spaces), but can fall short under large lag-expanded continuous settings where anytime compression is computationally safer.


## Dissertation Citation Traceability

- Chapter: `Introduction`; Section: `Chapter context (no explicit section)`; Line: `Chapters/01_introduction.tex:17`; Relevance: Cited to support the statement that A second challenge is interpretability under high-stakes use. Post hoc explanations of complex models can be useful, but interpretable-model-first approaches are often preferable when transparency, auditability, and operational trust are mandatory . Rule-structured models are especially relevant because they align with human review workflows and can be validated directly by domain experts . This dissertation therefore focuses on an inherently interpretable sparse pipeline rather than black-box prediction with after-the-fact explanation.
- Chapter: `Background`; Section: `Interpretability as a design requirement`; Line: `Chapters/02_background.tex:27`; Relevance: Cited to support the statement that In high-stakes settings, interpretability is not an afterthought: end-users must understand why a prediction was made and be able to verify it against domain knowledge. Interpretable-model-first arguments emphasize that transparent model classes are often preferable to post hoc explanations layered on top of black boxes . This principle is especially relevant when longitudinal decisions affect operations, safety, or clinical workflows.
- Chapter: `Related Work`; Section: `Interpretable Modeling and Rule-Learning Literature`; Line: `Chapters/03_relatedwork.tex:49`; Relevance: Cited to support the statement that Interpretability-focused literature increasingly argues that high-stakes deployments should prefer inherently interpretable models over post hoc explanations of black boxes whenever performance is competitive . This framing strongly influences the design choices in this work: interpretability is treated as a primary objective, not a secondary (post-hoc) reporting artifact.
- Chapter: `Research Questions`; Section: `Chapter context (no explicit section)`; Line: `Chapters/04_resquestions.tex:5`; Relevance: Cited to support the statement that This chapter defines the core research questions that guide this work and explains why they are important. Longitudinal feature learning is targeted under three simultaneous constraints: strong dependence created by lag expansion, the need for reliable feature-lag attribution, and the need for compact human-auditable model outputs. Prior chapters established that sparse methods are attractive but not always stable under dependence, and that interpretability requirements in high-stakes settings favor inherently transparent model forms over post hoc explanation layers.
- Chapter: `Research Questions`; Section: `Why RQ3 matters`; Line: `Chapters/04_resquestions.tex:34`; Relevance: Cited to support the statement that RQ3 addresses the final translational step: can sparse rectified models be converted into compact logical rules that humans can audit and use? In high-stakes settings, model acceptance often depends on transparency, verifiability, and ease of expert review, not only ROC-level discrimination.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `Why RQ3 Matters After RQ1 and RQ2`; Line: `Chapters/07_rq3.tex:11`; Relevance: Cited to support the statement that This is important for longitudinal and event-focused domains where users need a concrete trigger logic rather than a dense weight vector. In healthcare monitoring, safety surveillance, and maintenance settings, end users often need to know which few conditions jointly trigger escalation, and they need that logic in a form that is stable under repeated deployment . In interpretability terms, this chapter focuses on the transition from post hoc explanations to inherently interpretable rule structure.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `Relationship to Adjacent Interpretable Methods`; Line: `Chapters/07_rq3.tex:80`; Relevance: Cited to support the statement that Rule-list and scoring-system approaches provide strong interpretability baselines, but they often optimize directly in combinatorial rule space with different scalability profiles . The anytime compression strategy differs by leveraging a sparse convex fit first, then performing structured compression in a constrained post-processing stage. This separation is useful when the rectified feature space is large and sparse optimization is already part of the workflow.
- Chapter: `Future Work`; Section: `Defense-Readiness Objective`; Line: `Chapters/88_futurework.tex:16`; Relevance: Cited to support the statement that These criteria align with interpretable-ML guidance that model quality must include predictive performance, descriptive fidelity, and audience relevance for high-stakes use.
- Chapter: `Future Work`; Section: `Workstream C: RQ3 Anytime Compression Validation > C3. Interpretability quality audit`; Line: `Chapters/88_futurework.tex:84`; Relevance: Cited to support the statement that Rule models will be audited with explicit structural metrics: number of conditions, effective rule length, and decision-path simulatability. This follows interpretable-model guidance that transparent structure should be measured directly, not inferred from sparse coefficients alone.
- Chapter: `Conclusion`; Section: `Research Program Summary`; Line: `Chapters/89_conclusion.tex:16`; Relevance: Cited to support the statement that Together, these questions move from empirical behavior (RQ1), to theoretical plausibility (RQ2), to deployment usability (RQ3). This sequencing is intentional: interpretability claims are weak if support recovery is unstable, and practical rule compression is less meaningful without a credible upstream selection mechanism.
- Chapter: `Conclusion`; Section: `Conclusions by Research Question > RQ3 conclusion: Anytime compression makes interpretability operational`; Line: `Chapters/89_conclusion.tex:34`; Relevance: Cited to support the statement that For RQ3, this work concludes that sparse rectified models can often be compressed into substantially smaller rule representations while preserving practical discrimination quality under predefined tolerance policies . This directly addresses deployment requirements in settings where explanation, auditability, and compact decision logic are non-negotiable.
