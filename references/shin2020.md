# HAI 1.0: HIL-based Augmented ICS Security Dataset (Shin et al., 2020)

## Dissertation Alignment Context
The dissertation proposal uses a three-stage longitudinal pipeline: critical-range rectification of lagged signals into {-1,+1} indicators, L1-regularized logistic feature-and-lag selection, and anytime rule compression that converts sparse coefficients into compact m-of-K rules.

## Outline Summary of the Paper
1. **Motivation and dataset gap (Section 1):** The paper argues existing ICS/CPS security datasets are insufficient for robust AI-based anomaly detection research.
2. **HAI 1.0 contribution (Section 1):** It introduces HAI 1.0, a publicly released HIL-based augmented ICS dataset with long-duration normal and attack operations.
3. **Testbed architecture (Section 2):** The HAI platform integrates real boiler, turbine, and water-treatment subsystems with an HIL simulator to create richer inter-process dynamics.
4. **Process augmentation strategy (Section 2.1):** Signal-level coupling through the HIL model enables realistic hybrid power-process interactions and more diverse cyber-physical behaviors.
5. **Unmanned operation framework (Section 2.2):** Automated SCADA scheduling and setpoint variation support long, repeatable, low-human-effort data collection under controlled operating ranges.
6. **Attack methodology (Section 3.1):** Attack primitives are defined around process-control-loop variables (SP, CO, PV handling), including stealth behaviors via PV-response prevention.
7. **Attack scenario design (Section 3.2):** The dataset includes diverse single/multi-target scenarios with combinations of SP/CO manipulation and stealth modes, with explicit execution timelines.
8. **Data structure and labeling (Section 3.3):** HAI 1.0 provides normal and attack CSV splits with timestamped process points plus multi-level attack labels to support reproducible benchmarking.
9. **Evaluation recommendation (Section 3.4):** The paper recommends time-series-aware metrics (TaPR/eTaPR-style) instead of purely pointwise precision/recall to better assess anomaly detectors.
10. **Limitations and future roadmap (Section 4):** Authors plan improved transient-aware labeling and additional HAI versions with modified logic/sensors/actuators for cross-site generalization studies.

## Relevance to the Dissertation
HAI 1.0: HIL-based Augmented ICS Security Dataset (Shin et al., 2020) is relevant as benchmark infrastructure for testing longitudinal feature-lag attribution and rule interpretability.

## Elements from This Paper to Use in the Dissertation
1. Use dataset construction and protocol details to make dissertation experiments reproducible.
2. Adopt event definitions and temporal split logic when building lagged benchmarks.
3. Reuse context on hil-based, augmented, security for scenario-level interpretation and error analysis.
4. Use the dataset to compare attribution faithfulness across competing methods.

## Competitive Method Assessment
This paper does not present the main competing algorithmic pipeline. It is most useful for problem framing, benchmark selection, and identifying contexts where interpretable lag-aware rules are preferable to opaque alternatives.


## Dissertation Citation Traceability

- Chapter: `Introduction`; Section: `Contributions`; Line: `Chapters/01_introduction.tex:41`; Relevance: Cited to support the statement that item A broad empirical protocol for longitudinal evaluation. The dissertation compares raw and rectified representations across synthetic and real longitudinal benchmarks, including signal-rich settings such as ICS anomaly data and ionospheric radar returns, with evaluation focused on both discrimination and attribution quality.
- Chapter: `Related Work`; Section: `Longitudinal High-Dimensional Modeling Context`; Line: `Chapters/03_relatedwork.tex:63`; Relevance: Cited to support the statement that Benchmark infrastructure also matters for evaluating longitudinal methods. Public anomaly and signal datasets used in this line of inquiry, such as HAI ICS telemetry and ionospheric radar-return data, provide realistic stress tests for lag attribution, sparse recovery, and interpretability tradeoffs . Related engineering contexts with threshold-driven transitions likewise support the relevance of range-based temporal reasoning.
- Chapter: `Research Questions`; Section: `Cross-RQ Evaluation Priorities`; Line: `Chapters/04_resquestions.tex:44`; Relevance: Cited to support the statement that item Predictive discrimination: metrics such as AUC and Youden's J at operational thresholds, with context-dependent interpretation across datasets.
- Chapter: `Preliminary Studies and Evidence to Date for RQ1`; Section: `Real-world evidence and interpretability tradeoffs`; Line: `Chapters/05_rq1.tex:68`; Relevance: Cited to support the statement that Synthetic evidence is necessary but not sufficient. Preliminary real-world studies therefore evaluate whether the same pattern appears in operationally relevant longitudinal data. The HAI industrial control benchmark is particularly useful because it includes realistic multi-sensor temporal dynamics with labeled attack scenarios and known subsystem context . Additional evidence from historical ionospheric radar work supports the relevance of lagged signal discrimination settings where sparse attribution can complement raw predictive performance.
- Chapter: `Anytime Rule Compression (RQ3)`; Section: `Why RQ3 Matters After RQ1 and RQ2`; Line: `Chapters/07_rq3.tex:11`; Relevance: Cited to support the statement that This is important for longitudinal and event-focused domains where users need a concrete trigger logic rather than a dense weight vector. In healthcare monitoring, safety surveillance, and maintenance settings, end users often need to know which few conditions jointly trigger escalation, and they need that logic in a form that is stable under repeated deployment . In interpretability terms, this chapter focuses on the transition from post hoc explanations to inherently interpretable rule structure.
- Chapter: `Future Work`; Section: `Workstream A: RQ1 Empirical Strengthening > A3. Cross-domain longitudinal validation`; Line: `Chapters/88_futurework.tex:46`; Relevance: Cited to support the statement that At least one additional longitudinal setting will be included beyond current core experiments to test transferability of the threshold-and-lag assumptions. Candidate contexts include ICS anomaly detection and biomedical/clinical trajectory settings . A legacy signal-processing dataset is retained as a secondary check for lagged-feature behavior.
- Chapter: `Conclusion`; Section: `Practical Implications`; Line: `Chapters/89_conclusion.tex:69`; Relevance: Cited to support the statement that This work's practical implication is straightforward: when longitudinal decisions require both predictive utility and audit-ready rationale, it can be more effective to restructure features around event-relevant critical ranges first, then apply mature sparse optimization, and finally compress to explicit rule logic. This design is particularly relevant in domains such as ICS monitoring and clinical trajectory analysis, where lag effects, threshold triggers, and traceable decision criteria are operationally important.
